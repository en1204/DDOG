model:
  name: "meta-llama/Meta-Llama-3-8B-Instruct"
  device: "cuda"  # 或 "cpu"
  max_length: 512
  temperature: 0.7
  top_p: 0.9
  batch_size: 32

graph_retrieval:
  max_nodes: 100  
  max_edges: 500  
  min_node_score: 0.3  
  max_path_length: 5 


entity_alignment:
  similarity_threshold: 0.8 
  semantic_weight: 0.7  
  string_weight: 0.3  


path_filtering:
  reach_threshold: 0.7  
  alpha: 0.6  
  beta: 0.4  
  min_path_score: 0.5  


ercr:
  min_connectivity: 0.3  
  min_relevance: 0.4  
  max_path_length: 5 


answer_generation:
  max_new_tokens: 100
  num_beams: 4
  temperature: 0.7
  do_sample: true
  top_p: 0.9


evaluation:
  metrics:
    - "exact_match"  
    - "f1"          
    - "precision"   
    - "recall"     
    - "avg_num_tokens"  
  weights:
    exact_match: 0.3
    f1: 0.3
    precision: 0.2
    recall: 0.2


training:
  learning_rate: 1e-4
  num_epochs: 10
  warmup_steps: 1000
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  save_steps: 1000
  eval_steps: 500


data:
  train_path: "data/train.json"
  val_path: "data/val.json"
  test_path: "data/test.json"
  knowledge_base_path: "data/knowledge_base/"
  cache_dir: "cache/"
  output_dir: "output/"
  model_dir: "checkpoints/"

logging:
  level: "INFO"
  file: "logs/retrieval.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

experiment:
  seed: 4
  debug_mode: false
  save_predictions: true # 是否保存预测结果
  num_workers: 4 